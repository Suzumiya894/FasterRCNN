from . import vgg16 

import numpy as np
import numpy as xp
import six

from math import log

from collections import defaultdict

from . import region_proposal_network


def compute_rpn_maps(input_image_shape, ground_truth_object_boxes):
  ratios=[0.5, 1, 2]
  anchor_scales=[8, 16, 32]
  num_anchors = len(ratios) * len(anchor_scales)
  img_size = input_image_shape[0], input_image_shape[1] # height, width
  anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios)
  height, width = vgg16.compute_output_map_shape(input_image_shape = input_image_shape)
  anchor = _enumerate_shifted_anchor(np.array(anchor_base), 16, height, width)
  anchor_target_creator = AnchorTargetCreator()

  # Convert ground truth object box coordinates to [N,4] tensor
  bbox = np.zeros((len(ground_truth_object_boxes), 4))
  for i in range(len(ground_truth_object_boxes)):
    bbox[i,:] = ground_truth_object_boxes[i].corners
  gt_rpn_anchor_boxes, gt_rpn_label, targets = anchor_target_creator(bbox, anchor, img_size)

  # Create anchor map and anchor valid map
  """
  anchor_boxes = np.zeros((height, width, num_anchors * 4))
  for y in range(height):
    for x in range(width):
      for k in range(num_anchors):
        h, w = anchor_index_to_size(k)
        center_x = (x + 0.5) * 16
        center_y = (y + 0.5) * 16
        anchor_boxes[y,x,k*4+0:k*4+4] = (center_y, center_x, h, w)
  """
  anchor_boxes, _ = region_proposal_network.compute_all_anchor_boxes(input_image_shape = input_image_shape)

  # Create truth map
  anchor_boxes_valid = np.zeros((height, width, num_anchors))
  truth_map = np.zeros((height, width, num_anchors, 8))
  truth_map[:,:,:,0] = 0  # invalid
  for i in range(gt_rpn_anchor_boxes.shape[0]):
    anchor_center_y = 0.5 * (gt_rpn_anchor_boxes[i,0] + gt_rpn_anchor_boxes[i,2])
    anchor_center_x = 0.5 * (gt_rpn_anchor_boxes[i,1] + gt_rpn_anchor_boxes[i,3])
    anchor_height = gt_rpn_anchor_boxes[i,2] - gt_rpn_anchor_boxes[i,0]
    anchor_width = gt_rpn_anchor_boxes[i,3] - gt_rpn_anchor_boxes[i,1]
    y = int(anchor_center_y//16)#int(np.round(anchor_center_y / 16))
    x = int(anchor_center_x//16)#int(np.round(anchor_center_x / 16))
    k = anchor_size_to_index(anchor_height, anchor_width)
    if gt_rpn_label[i] > 0:
      # Positive anchor
      anchor_boxes_valid[y,x,k] = 1 # valid
      truth_map[y,x,k,0] = 1  # valid
      truth_map[y,x,k,1] = 1  # positive
      truth_map[y,x,k,2] = 1 + 0  # we don't actually have the correct label here but it's not used anywhere anyway 

      # Regression targets
      truth_map[y,x,k,4:8] = targets[i,:]
    elif gt_rpn_label[i] == 0:
      # Negative anchor
      anchor_boxes_valid[y,x,k] = 1 # valid
      truth_map[y,x,k,0] = 1
      truth_map[y,x,k,1] = 0
      truth_map[y,x,k,2] = -1

  # Get positive and negative anchors
  height, width, num_anchors, _ = truth_map.shape
  truth_map_coords = np.transpose(np.mgrid[0:height,0:width,0:num_anchors], (1,2,3,0))  # shape (height,width,k,3): every index (y,x,k,:) returns its own coordinate (y,x,k)
  object_anchors = truth_map_coords[np.where(truth_map[:,:,:,2] > 0)]                   # shape (N,3), where each row is the coordinate (y,x,k) of a positive sample
  not_object_anchors = truth_map_coords[np.where(truth_map[:,:,:,2] < 0)]               # shape (N,3), where each row is the coordinate (y,x,k) of a negative sample
  return truth_map, anchor_boxes, anchor_boxes_valid, object_anchors, not_object_anchors

def anchor_size_to_index(h, w):
  """
  Mapping of index k <-> anchor height, width
  0 90.50966799187809 181.01933598375618
  1 181.01933598375618 362.03867196751236
  2 362.03867196751236 724.0773439350247
  3 128.0 128.0
  4 256.0 256.0
  5 512.0 512.0
  6 181.01933598375618 90.50966799187809
  7 362.03867196751236 181.01933598375618
  8 724.0773439350247 362.03867196751236
  """
  k_by_size = {
    (90,181): 0,
    (181,362): 1,
    (362,724): 2,
    (128,128): 3,
    (256,256): 4,
    (512,512): 5,
    (181,90): 6,
    (362,181): 7,
    (724,362): 8,
  }
  return k_by_size[(int(h), int(w))]
    
def anchor_index_to_size(k):
  size_by_k = {
    0: (90.50966799187809, 181.01933598375618),
    1: (181.01933598375618, 362.03867196751236),
    2: (362.03867196751236, 724.0773439350247),
    3: (128.0, 128.0),
    4: (256.0, 256.0),
    5: (512.0, 512.0),
    6: (181.01933598375618, 90.50966799187809),
    7: (362.03867196751236, 181.01933598375618),
    8: (724.0773439350247, 362.03867196751236)
  }
  return size_by_k[k]

def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],
                         anchor_scales=[8, 16, 32]):
    """Generate anchor base windows by enumerating aspect ratio and scales.

    Generate anchors that are scaled and modified to the given aspect ratios.
    Area of a scaled anchor is preserved when modifying to the given aspect
    ratio.

    :obj:`R = len(ratios) * len(anchor_scales)` anchors are generated by this
    function.
    The :obj:`i * len(anchor_scales) + j` th anchor corresponds to an anchor
    generated by :obj:`ratios[i]` and :obj:`anchor_scales[j]`.

    For example, if the scale is :math:`8` and the ratio is :math:`0.25`,
    the width and the height of the base window will be stretched by :math:`8`.
    For modifying the anchor to the given aspect ratio,
    the height is halved and the width is doubled.

    Args:
        base_size (number): The width and the height of the reference window.
        ratios (list of floats): This is ratios of width to height of
            the anchors.
        anchor_scales (list of numbers): This is areas of anchors.
            Those areas will be the product of the square of an element in
            :obj:`anchor_scales` and the original area of the reference
            window.

    Returns:
        ~numpy.ndarray:
        An array of shape :math:`(R, 4)`.
        Each element is a set of coordinates of a bounding box.
        The second axis corresponds to
        :math:`(y_{min}, x_{min}, y_{max}, x_{max})` of a bounding box.

    """
    py = base_size / 2.
    px = base_size / 2.

    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),
                           dtype=np.float32)
    for i in six.moves.range(len(ratios)):
        for j in six.moves.range(len(anchor_scales)):
            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])
            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])
            index = i * len(anchor_scales) + j
            
            anchor_base[index, 0] = py - h / 2.
            anchor_base[index, 1] = px - w / 2.
            anchor_base[index, 2] = py + h / 2.
            anchor_base[index, 3] = px + w / 2.

    return anchor_base

def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):
    # Enumerate all shifted anchors:
    #
    # add A anchors (1, A, 4) to
    # cell K shifts (K, 1, 4) to get
    # shift anchors (K, A, 4)
    # reshape to (K*A, 4) shifted anchors
    # return (K*A, 4)
 
    # !TODO: add support for torch.CudaTensor
    # xp = cuda.get_array_module(anchor_base)
    # it seems that it can't be boosed using GPU
    import numpy as xp
    shift_y = xp.arange(0, height * feat_stride, feat_stride)
    shift_x = xp.arange(0, width * feat_stride, feat_stride)
    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)
    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),
                      shift_y.ravel(), shift_x.ravel()), axis=1)

    A = anchor_base.shape[0]
    K = shift.shape[0]
    anchor = anchor_base.reshape((1, A, 4)) + \
             shift.reshape((1, K, 4)).transpose((1, 0, 2))
    anchor = anchor.reshape((K * A, 4)).astype(np.float32)
    return anchor



class AnchorTargetCreator(object):
    """Assign the ground truth bounding boxes to anchors.

    Assigns the ground truth bounding boxes to anchors for training Region
    Proposal Networks introduced in Faster R-CNN [#]_.

    Offsets and scales to match anchors to the ground truth are
    calculated using the encoding scheme of
    :func:`model.utils.bbox_tools.bbox2loc`.

    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \
    Faster R-CNN: Towards Real-Time Object Detection with \
    Region Proposal Networks. NIPS 2015.

    Args:
        n_sample (int): The number of regions to produce.
        pos_iou_thresh (float): Anchors with IoU above this
            threshold will be assigned as positive.
        neg_iou_thresh (float): Anchors with IoU below this
            threshold will be assigned as negative.
        pos_ratio (float): Ratio of positive regions in the
            sampled regions.

    """

    def __init__(self,
                 n_sample=256,
                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,
                 pos_ratio=0.5):
        self.n_sample = n_sample
        self.pos_iou_thresh = pos_iou_thresh
        self.neg_iou_thresh = neg_iou_thresh
        self.pos_ratio = pos_ratio

    def __call__(self, bbox, anchor, img_size):
        """Assign ground truth supervision to sampled subset of anchors.

        Types of input arrays and output arrays are same.

        Here are notations.

        * :math:`S` is the number of anchors.
        * :math:`R` is the number of bounding boxes.

        Args:
            bbox (array): Coordinates of bounding boxes. Its shape is
                :math:`(R, 4)`.
            anchor (array): Coordinates of anchors. Its shape is
                :math:`(S, 4)`.
            img_size (tuple of ints): A tuple :obj:`H, W`, which
                is a tuple of height and width of an image.

        Returns:
            (array, array):

            #NOTE: it's scale not only  offset
            * **loc**: Offsets and scales to match the anchors to \
                the ground truth bounding boxes. Its shape is :math:`(S, 4)`.
            * **label**: Labels of anchors with values \
                :obj:`(1=positive, 0=negative, -1=ignore)`. Its shape \
                is :math:`(S,)`.

        """

        img_H, img_W = img_size

        n_anchor = len(anchor)
        inside_index = _get_inside_index(anchor, img_H, img_W)
        anchor = anchor[inside_index]
        argmax_ious, label = self._create_label(
            inside_index, anchor, bbox)

        return anchor, label, bbox2loc(anchor, bbox[argmax_ious])


        # compute bounding box regression targets
        loc = bbox2loc(anchor, bbox[argmax_ious])
        
        # map up to original set of anchors
        label = _unmap(label, n_anchor, inside_index, fill=-1)
        loc = _unmap(loc, n_anchor, inside_index, fill=0)

        return loc, label

    def _create_label(self, inside_index, anchor, bbox):
        # label: 1 is positive, 0 is negative, -1 is dont care
        label = np.empty((len(inside_index),), dtype=np.int32)
        label.fill(-1)

        argmax_ious, max_ious, gt_argmax_ious = \
            self._calc_ious(anchor, bbox, inside_index)

        # assign negative labels first so that positive labels can clobber them
        label[max_ious < self.neg_iou_thresh] = 0

        # positive label: for each gt, anchor with highest iou
        label[gt_argmax_ious] = 1

        # positive label: above threshold IOU
        label[max_ious >= self.pos_iou_thresh] = 1
        
        """
        # subsample positive labels if we have too many
        n_pos = int(self.pos_ratio * self.n_sample)
        pos_index = np.where(label == 1)[0]
        if len(pos_index) > n_pos:
            disable_index = np.random.choice(
                pos_index, size=(len(pos_index) - n_pos), replace=False)
            label[disable_index] = -1

        # subsample negative labels if we have too many
        n_neg = self.n_sample - np.sum(label == 1)
        neg_index = np.where(label == 0)[0]
        if len(neg_index) > n_neg:
            disable_index = np.random.choice(
                neg_index, size=(len(neg_index) - n_neg), replace=False)
            label[disable_index] = -1
        """

        # Print anchors and IoUs
        """
        for i in range(bbox.shape[0]):
          print("Box %d:", bbox[i])
        print(anchor.shape)
        print(max_ious.shape)
        print(argmax_ious.shape)
        print(gt_argmax_ious.shape)
        print("max ious=", np.max(max_ious))
        for i in range(anchor.shape[0]):
          height = anchor[i,2] - anchor[i,0]
          width = anchor[i,3] - anchor[i,1]
          cy = 0.5 * (anchor[i,0] + anchor[i,2])
          cx = 0.5 * (anchor[i,1] + anchor[i,3])
          print("*** %d (%d,%d) -- %f x %f -- %f %f ---- (%f,%f,%f,%f)" % (i, cx, cy, width, height, max_ious[i], argmax_ious[i], anchor[i,0], anchor[i,1], anchor[i,2], anchor[i,3]))
        """

        return argmax_ious, label

    def _calc_ious(self, anchor, bbox, inside_index):
        # ious between the anchors and the gt boxes
        ious = bbox_iou(anchor, bbox)
        argmax_ious = ious.argmax(axis=1)
        max_ious = ious[np.arange(len(inside_index)), argmax_ious]
        gt_argmax_ious = ious.argmax(axis=0)
        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]
        gt_argmax_ious = np.where(ious == gt_max_ious)[0]

        """
        #TODO: need to account for variable number of GT boxes instead of assuming two
        for i in range(len(ious)):
          cy = 0.5 * (anchor[i,0] + anchor[i,2])
          cx = 0.5 * (anchor[i,1] + anchor[i,3])
          h = anchor[i,2] - anchor[i,0]
          w = anchor[i,3] - anchor[i,1]
          print("(%d,%d) %f x %f -> (%f,%f)" % (cy, cx, h, w, ious[i,0], ious[i,1]))
        """

        return argmax_ious, max_ious, gt_argmax_ious

def _unmap(data, count, index, fill=0):
    # Unmap a subset of item (data) back to the original set of items (of
    # size count)

    if len(data.shape) == 1:
        ret = np.empty((count,), dtype=data.dtype)
        ret.fill(fill)
        ret[index] = data
    else:
        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)
        ret.fill(fill)
        ret[index, :] = data
    return ret


def _get_inside_index(anchor, H, W):
    # Calc indicies of anchors which are located completely inside of the image
    # whose size is speficied.
    index_inside = np.where(
        (anchor[:, 0] >= 0) &
        (anchor[:, 1] >= 0) &
        (anchor[:, 2] <= H) &
        (anchor[:, 3] <= W)
    )[0]
    return index_inside

def bbox_iou(bbox_a, bbox_b):
    """Calculate the Intersection of Unions (IoUs) between bounding boxes.

    IoU is calculated as a ratio of area of the intersection
    and area of the union.

    This function accepts both :obj:`numpy.ndarray` and :obj:`cupy.ndarray` as
    inputs. Please note that both :obj:`bbox_a` and :obj:`bbox_b` need to be
    same type.
    The output is same type as the type of the inputs.

    Args:
        bbox_a (array): An array whose shape is :math:`(N, 4)`.
            :math:`N` is the number of bounding boxes.
            The dtype should be :obj:`numpy.float32`.
        bbox_b (array): An array similar to :obj:`bbox_a`,
            whose shape is :math:`(K, 4)`.
            The dtype should be :obj:`numpy.float32`.

    Returns:
        array:
        An array whose shape is :math:`(N, K)`. \
        An element at index :math:`(n, k)` contains IoUs between \
        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \
        box in :obj:`bbox_b`.

    """
    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:
        raise IndexError

    # top left
    tl = xp.maximum(bbox_a[:, None, :2], bbox_b[:, :2])
    # bottom right
    br = xp.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])

    area_i = xp.prod(br - tl, axis=2) * (tl < br).all(axis=2)
    area_a = xp.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)
    area_b = xp.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)
    return area_i / (area_a[:, None] + area_b - area_i)


def bbox2loc(src_bbox, dst_bbox):
    """Encodes the source and the destination bounding boxes to "loc".

    Given bounding boxes, this function computes offsets and scales
    to match the source bounding boxes to the target bounding boxes.
    Mathematcially, given a bounding box whose center is
    :math:`(y, x) = p_y, p_x` and
    size :math:`p_h, p_w` and the target bounding box whose center is
    :math:`g_y, g_x` and size :math:`g_h, g_w`, the offsets and scales
    :math:`t_y, t_x, t_h, t_w` can be computed by the following formulas.

    * :math:`t_y = \\frac{(g_y - p_y)} {p_h}`
    * :math:`t_x = \\frac{(g_x - p_x)} {p_w}`
    * :math:`t_h = \\log(\\frac{g_h} {p_h})`
    * :math:`t_w = \\log(\\frac{g_w} {p_w})`

    The output is same type as the type of the inputs.
    The encoding formulas are used in works such as R-CNN [#]_.

    .. [#] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. \
    Rich feature hierarchies for accurate object detection and semantic \
    segmentation. CVPR 2014.

    Args:
        src_bbox (array): An image coordinate array whose shape is
            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.
            These coordinates are
            :math:`p_{ymin}, p_{xmin}, p_{ymax}, p_{xmax}`.
        dst_bbox (array): An image coordinate array whose shape is
            :math:`(R, 4)`.
            These coordinates are
            :math:`g_{ymin}, g_{xmin}, g_{ymax}, g_{xmax}`.

    Returns:
        array:
        Bounding box offsets and scales from :obj:`src_bbox` \
        to :obj:`dst_bbox`. \
        This has shape :math:`(R, 4)`.
        The second axis contains four values :math:`t_y, t_x, t_h, t_w`.

    """

    height = src_bbox[:, 2] - src_bbox[:, 0]
    width = src_bbox[:, 3] - src_bbox[:, 1]
    ctr_y = src_bbox[:, 0] + 0.5 * height
    ctr_x = src_bbox[:, 1] + 0.5 * width

    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]
    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]
    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height
    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width

    eps = xp.finfo(height.dtype).eps
    height = xp.maximum(height, eps)
    width = xp.maximum(width, eps)

    dy = (base_ctr_y - ctr_y) / height
    dx = (base_ctr_x - ctr_x) / width
    dh = xp.log(base_height / height)
    dw = xp.log(base_width / width)

    loc = xp.vstack((dy, dx, dh, dw)).transpose()
    return loc
